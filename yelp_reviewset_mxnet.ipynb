{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#before running the notebook, download yelp dataset from https://www.yelp.com/dataset/challenge\n",
    "#untar the dataset with the following command tar -xvf yelp_dataset_challenge_round9.tgz \n",
    "#in the same directory as this notebook. \n",
    "import copy\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import json\n",
    "from text import Tokenizer\n",
    "import mxnet as mx\n",
    "from matplotlib import pyplot\n",
    "from six.moves.urllib.request import urlopen\n",
    "from sequence import pad_sequences\n",
    "\n",
    "from IPython.display import display \n",
    "from IPython.html import widgets\n",
    "\n",
    "# Enable logging so we will see output during the training\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:52.357759\n"
     ]
    }
   ],
   "source": [
    "# Load the reviews and parse JSON\n",
    "t1 = datetime.now()\n",
    "with open(\"yelp_academic_dataset_review.json\") as f:\n",
    "    reviews = f.read().strip().split(\"\\n\")\n",
    "reviews = [json.loads(review) for review in reviews]\n",
    "print(datetime.now() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [review['text'] for review in reviews]\n",
    " \n",
    "# Convert our 5 classes into 2 (negative or positive)\n",
    "binstars = [0 if review['stars'] <= 3 else 1 for review in reviews]\n",
    "balanced_texts = []\n",
    "balanced_labels = []\n",
    "balanced_texts_test = []\n",
    "balanced_labels_test = []\n",
    "limit = 50000  # Change this to grow/shrink the dataset\n",
    "neg_pos_counts_train = [0, 0]\n",
    "neg_pos_counts_test = [0, 0]\n",
    "for i in range(len(texts)):\n",
    "    polarity = binstars[i]\n",
    "    text = texts[i].encode('utf-8')\n",
    "    if neg_pos_counts_train[polarity] < limit:\n",
    "        balanced_texts.append(text)\n",
    "        balanced_labels.append(binstars[i])\n",
    "        neg_pos_counts_train[polarity] += 1\n",
    "    elif neg_pos_counts_test[polarity] < limit:\n",
    "        balanced_texts_test.append(text)\n",
    "        balanced_labels_test.append(binstars[i])\n",
    "        neg_pos_counts_test[polarity] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 50000, 1: 50000})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(balanced_labels)\n",
    "Counter(balanced_labels_test)\n",
    "# >>> Counter({0: 100000, 1: 100000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_words=10000\n",
    "tokenizer = Tokenizer(num_words)\n",
    "tokenizer.fit_on_texts(balanced_texts)\n",
    "\n",
    "balanced_texts = tokenizer.texts_to_sequences(balanced_texts)\n",
    "balanced_texts_test  = tokenizer.texts_to_sequences(balanced_texts_test)\n",
    "\n",
    "vocabsize = num_words\n",
    "X = np.concatenate((balanced_texts, balanced_texts_test), axis=0)\n",
    "\n",
    "# Specify the maximum length of the reviews we want to process and pad the training and test data \n",
    "maxtextlen = 500\n",
    "X_train = pad_sequences(balanced_texts, maxlen=maxtextlen)\n",
    "X_test = pad_sequences(balanced_texts_test, maxlen=maxtextlen)\n",
    "\n",
    "\n",
    "# convert list to nd array type as mx.io.NDArrayIter takes nd array data type\n",
    "y_train = np.asarray(balanced_labels)\n",
    "y_test = np.asarray(balanced_labels_test)\n",
    "\n",
    "# Create MXNet NDArray Iterators from the numpy training set and labels.  A batch size specified and the data will\n",
    "# be shffled.  The iterators will be used as input to train and measure the model performance later.\n",
    "Batch_Size = 250\n",
    "train_iter = mx.io.NDArrayIter(X_train, y_train, Batch_Size, shuffle=True)\n",
    "test_iter = mx.io.NDArrayIter(X_test, y_test, Batch_Size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words : 9999\n",
      "\n",
      "Label value\n",
      "[0 1]\n",
      "\n",
      "Review length: \n",
      "Mean 119.93 words (107.299532)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVCkDIEmVMgCSVCkDIEmVMgCSVKn/B6ZzrWi6xC7BAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faefdda42d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's do some analysis of the data\n",
    "# Summarize review length\n",
    "print(\"Number of unique words : %i\" % len(np.unique(np.hstack(X))))\n",
    "print ('')\n",
    "print (\"Label value\")\n",
    "print (np.unique(y_train))\n",
    "print ('')\n",
    "print(\"Review length: \")\n",
    "\n",
    "result = [len(x) for x in X]\n",
    "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
    "\n",
    "\n",
    "# plot review length distribution\n",
    "pyplot.boxplot(result)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Example - Coded with word index\n",
      "[ list([33, 17, 514, 58, 73, 337, 103, 9, 28, 4345, 28, 57, 9, 2217, 3, 52, 184, 406, 1, 51, 57, 78, 794, 45, 78, 66, 37, 2, 382, 16, 207, 6213, 2, 2045, 3, 713, 21, 5, 182, 207, 125, 179, 15, 9, 406, 7656, 18, 1782, 57, 9, 31, 33, 3, 74, 9, 37, 83, 73, 1, 122, 84, 21, 41, 3071, 1867, 18, 1438, 1971])]\n"
     ]
    }
   ],
   "source": [
    "# Let's also take a look at 1 row of the training data\n",
    "# The integers represent a word in the original text \n",
    "print ('Review Example - Coded with word index')\n",
    "print (X[0:1, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout probability 0.0\n"
     ]
    }
   ],
   "source": [
    "# create MLP network using MXNet Symbol API\n",
    "\n",
    "# Create the input layer and place holder for the label\n",
    "inputdata = mx.sym.Variable('data')\n",
    "input_y = mx.sym.Variable('softmax_label')  # placeholder for label\n",
    "\n",
    "# We embed the integer representation for each word into a vector of size 32.  Embedding is a technique that help \n",
    "# place related words close together. This helps improve the accuracy of model\n",
    "# input_dim is the size of the vocabulary.  output_dim is the dimension of the output embedded vector.\n",
    "Embeddata = mx.sym.Embedding(data = inputdata, input_dim=vocabsize, output_dim=32, name='embed') \n",
    "\n",
    "# The output from the embedding layer will be dimensional matrix, since MLP only accepts 1 dimensional vector, \n",
    "# we need to flatten it back to one dimension vector\n",
    "data1 = mx.sym.Flatten(data = Embeddata, name='flatten')\n",
    "\n",
    "\n",
    "# We create a fully connected layer with 250 neurons.  This layer will take the flattened input and \n",
    "# perform a linear calculation on the input data f(x) = ⟨w, x⟩ + b\n",
    "fc1  = mx.sym.FullyConnected(data=data1, num_hidden=250)\n",
    "\n",
    "\n",
    "# We add some nonlearity (Activation) into the network, so we can model non linear data patterns as not problem is linear problem\n",
    "# Some of the common activations functions are 'relu', 'tanh', sigmoid.  \n",
    "act1 = mx.sym.Activation(data=fc1, act_type=\"relu\")  \n",
    "\n",
    "# We create anothe hidden layer with 2 hidden units as we have 2 desired output (1, 0)\n",
    "fc2 = mx.sym.FullyConnected(data=act1, num_hidden=2) \n",
    "\n",
    "# Softmax is a classifier, and cross-entropy loss is used as the loss function by default.  \n",
    "mlp = mx.sym.SoftmaxOutput(data=fc2, label=input_y, name='softmax')\n",
    "\n",
    "# Now we have completed building the network, let's see what it looks like\n",
    "#mx.viz.plot_network(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[0] Batch [100]\tSpeed: 9549.54 samples/sec\taccuracy=0.769347\n",
      "INFO:root:Epoch[0] Batch [200]\tSpeed: 10183.15 samples/sec\taccuracy=0.846440\n",
      "INFO:root:Epoch[0] Batch [300]\tSpeed: 10167.72 samples/sec\taccuracy=0.848440\n",
      "INFO:root:Epoch[0] Train-accuracy=0.849778\n",
      "INFO:root:Epoch[0] Time cost=10.037\n",
      "INFO:root:Epoch[0] Validation-accuracy=0.864370\n",
      "INFO:root:Epoch[1] Batch [100]\tSpeed: 10194.13 samples/sec\taccuracy=0.881188\n",
      "INFO:root:Epoch[1] Batch [200]\tSpeed: 10191.89 samples/sec\taccuracy=0.898400\n",
      "INFO:root:Epoch[1] Batch [300]\tSpeed: 10173.86 samples/sec\taccuracy=0.887360\n",
      "INFO:root:Epoch[1] Train-accuracy=0.886101\n",
      "INFO:root:Epoch[1] Time cost=9.800\n",
      "INFO:root:Epoch[1] Validation-accuracy=0.844830\n",
      "INFO:root:Epoch[2] Batch [100]\tSpeed: 9832.27 samples/sec\taccuracy=0.919010\n",
      "INFO:root:Epoch[2] Batch [200]\tSpeed: 10182.43 samples/sec\taccuracy=0.942040\n",
      "INFO:root:Epoch[2] Batch [300]\tSpeed: 10162.83 samples/sec\taccuracy=0.928280\n",
      "INFO:root:Epoch[2] Train-accuracy=0.925091\n",
      "INFO:root:Epoch[2] Time cost=9.894\n",
      "INFO:root:Epoch[2] Validation-accuracy=0.823150\n",
      "INFO:root:Epoch[3] Batch [100]\tSpeed: 10187.14 samples/sec\taccuracy=0.951366\n",
      "INFO:root:Epoch[3] Batch [200]\tSpeed: 10182.83 samples/sec\taccuracy=0.965040\n",
      "INFO:root:Epoch[3] Batch [300]\tSpeed: 10161.59 samples/sec\taccuracy=0.953360\n",
      "INFO:root:Epoch[3] Train-accuracy=0.951192\n",
      "INFO:root:Epoch[3] Time cost=9.807\n",
      "INFO:root:Epoch[3] Validation-accuracy=0.832210\n",
      "INFO:root:Epoch[4] Batch [100]\tSpeed: 10179.65 samples/sec\taccuracy=0.953267\n",
      "INFO:root:Epoch[4] Batch [200]\tSpeed: 10180.13 samples/sec\taccuracy=0.961920\n",
      "INFO:root:Epoch[4] Batch [300]\tSpeed: 10162.60 samples/sec\taccuracy=0.966640\n",
      "INFO:root:Epoch[4] Train-accuracy=0.961253\n",
      "INFO:root:Epoch[4] Time cost=9.810\n",
      "INFO:root:Epoch[4] Validation-accuracy=0.823150\n",
      "INFO:root:Epoch[5] Batch [100]\tSpeed: 10176.28 samples/sec\taccuracy=0.974337\n",
      "INFO:root:Epoch[5] Batch [200]\tSpeed: 10177.58 samples/sec\taccuracy=0.984520\n",
      "INFO:root:Epoch[5] Batch [300]\tSpeed: 10152.55 samples/sec\taccuracy=0.979120\n",
      "INFO:root:Epoch[5] Train-accuracy=0.979354\n",
      "INFO:root:Epoch[5] Time cost=9.816\n",
      "INFO:root:Epoch[5] Validation-accuracy=0.814930\n",
      "INFO:root:Epoch[6] Batch [100]\tSpeed: 9821.71 samples/sec\taccuracy=0.979446\n",
      "INFO:root:Epoch[6] Batch [200]\tSpeed: 10171.56 samples/sec\taccuracy=0.988560\n",
      "INFO:root:Epoch[6] Batch [300]\tSpeed: 10154.78 samples/sec\taccuracy=0.985000\n",
      "INFO:root:Epoch[6] Train-accuracy=0.982182\n",
      "INFO:root:Epoch[6] Time cost=9.905\n",
      "INFO:root:Epoch[6] Validation-accuracy=0.828380\n",
      "INFO:root:Epoch[7] Batch [100]\tSpeed: 9824.77 samples/sec\taccuracy=0.984634\n",
      "INFO:root:Epoch[7] Batch [200]\tSpeed: 10178.22 samples/sec\taccuracy=0.990200\n",
      "INFO:root:Epoch[7] Batch [300]\tSpeed: 10155.80 samples/sec\taccuracy=0.985960\n",
      "INFO:root:Epoch[7] Train-accuracy=0.987636\n",
      "INFO:root:Epoch[7] Time cost=9.901\n",
      "INFO:root:Epoch[7] Validation-accuracy=0.824910\n",
      "INFO:root:Epoch[8] Batch [100]\tSpeed: 10175.45 samples/sec\taccuracy=0.988911\n",
      "INFO:root:Epoch[8] Batch [200]\tSpeed: 10176.34 samples/sec\taccuracy=0.990920\n",
      "INFO:root:Epoch[8] Batch [300]\tSpeed: 10156.30 samples/sec\taccuracy=0.990720\n",
      "INFO:root:Epoch[8] Train-accuracy=0.988323\n",
      "INFO:root:Epoch[8] Time cost=9.815\n",
      "INFO:root:Epoch[8] Validation-accuracy=0.824090\n",
      "INFO:root:Epoch[9] Batch [100]\tSpeed: 10172.15 samples/sec\taccuracy=0.990693\n",
      "INFO:root:Epoch[9] Batch [200]\tSpeed: 10163.32 samples/sec\taccuracy=0.990080\n",
      "INFO:root:Epoch[9] Batch [300]\tSpeed: 10157.60 samples/sec\taccuracy=0.992200\n",
      "INFO:root:Epoch[9] Train-accuracy=0.991515\n",
      "INFO:root:Epoch[9] Time cost=9.819\n",
      "INFO:root:Epoch[9] Validation-accuracy=0.826280\n"
     ]
    }
   ],
   "source": [
    "# Set the number of epochs to run\n",
    "num_epoch = 10\n",
    "\n",
    "# Assign the network symbol(mlp) to the module class and we will use gpu here.  If cpu is used, then change it \n",
    "ctx = mx.gpu(0)\n",
    "mlp_model = mx.mod.Module(symbol=mlp, context=ctx) \n",
    "\n",
    "# Start training by calling the fit function\n",
    "mlp_model.fit(train_iter,  # training data               \n",
    "    eval_data=test_iter,  # validation data                            \n",
    "    optimizer=\"adam\",  # use adam optimizer to train\n",
    "    optimizer_params={'learning_rate':0.01}, # set learning rate for adam         \n",
    "    eval_metric='acc',  # report accuracy during training  \n",
    "    batch_end_callback = mx.callback.Speedometer(Batch_Size, 100), # output progress for each 100 data batches   \n",
    "    num_epoch=num_epoch) # train data passes indicatd by num_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch: data shapes: [(250L, 500L)] label shapes: [(250L,)]\n"
     ]
    }
   ],
   "source": [
    "test_iter.reset()\n",
    "print test_iter.next()\n",
    "test_iter.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mse', 0.48431988321244718), ('accuracy', 0.82628)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = mx.metric.Accuracy()\n",
    "mlp_model.score(test_iter, metric)\n",
    "mlp_model.score(test_iter, ['mse', 'acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved checkpoint to \"sentiment_mlp-0010.params\"\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "prefix = \"sentiment_mlp\"\n",
    "mlp_model.save_checkpoint (prefix, num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/mxnet-0.10.0-py2.7.egg/mxnet/module/base_module.py:64: UserWarning: Data provided by label_shapes don't match names specified by label_names ([] vs. ['softmax_label'])\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Let's make some prediction using the saved model\n",
    "# First load the model\n",
    "prefix = \"sentiment_mlp\"\n",
    "model = mx.mod.Module.load(prefix, num_epoch, False)\n",
    "\n",
    "# Now we need to bind the model with a datashape that represents the input, which will be 1xmaxtextlen\n",
    "model.bind(for_training=False, data_shapes=[('data', (1,maxtextlen))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some helper function for making the prediction\n",
    "\n",
    "# This function takes a text string and return a nd array with word indexes \n",
    "def prepare_imdb_list(text, maxlen=500, vocabsize=10000):\n",
    "    imdb_word_index = tokenizer.word_index\n",
    "    \n",
    "    sentence = []\n",
    "\n",
    "    sentence.append(str(text))\n",
    "    \n",
    "\n",
    "    #tokenize the input sentence\n",
    "    tokens = Tokenizer()\n",
    "    tokens.fit_on_texts(sentence)\n",
    "\n",
    "    # get a list of words from the encoding\n",
    "    words = []\n",
    "    for iter in range(len(tokens.word_index)):\n",
    "        words += [key for key,value in tokens.word_index.items() if value==iter+1]\n",
    "    \n",
    "    # create a imdb based sequence from the words and specified vocab size\n",
    "    imdb_seq = []\n",
    "    for w in words:\n",
    "        idx = imdb_word_index[w]\n",
    "        if idx < vocabsize:\n",
    "            imdb_seq.append(idx)\n",
    "\n",
    "    # next we need to create a list of list so we can use pad_sequence to pad the inputs\n",
    "    new_list = []\n",
    "    new_list.append(imdb_seq)\n",
    "\n",
    "    new_list = pad_sequences(new_list, maxlen=maxlen)\n",
    "    \n",
    "    return new_list\n",
    "\n",
    "\n",
    "def predict_sentiment(model, text_nd):\n",
    "    sentence_Iter = mx.io.NDArrayIter(text_nd, batch_size=1)\n",
    "    pred = model.predict(sentence_Iter)\n",
    "\n",
    "    return pred\n",
    "\n",
    "def handle_submit(sender):\n",
    "    text_nd = prepare_imdb_list(inputtext.value)\n",
    "    pred = predict_sentiment(model, text_nd)\n",
    "    outputlabel_0.value = 'Probability for negative sentiment (0):  %0.4f ' % pred.asnumpy()[0:1,0]\n",
    "    outputlabel_1.value = 'Probability for positive sentiment (1):   %0.4f ' % pred.asnumpy()[0:1,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d83d9d62f34f74b0bdd53d50106fe1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95421c55e41491fb73c75ba84509839"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d357e477f54e0d99f5e9fdc9a84f02"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb600dc52ae74976898cd1306abf3438"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputtext = widgets.Textarea()\n",
    "\n",
    "display(inputtext)\n",
    "\n",
    "inputbutton = widgets.Button(description='Predict Sentiment')\n",
    "\n",
    "display(inputbutton)\n",
    "\n",
    "outputlabel_0 = widgets.HTML()\n",
    "outputlabel_1 = widgets.HTML()\n",
    "display(outputlabel_0)\n",
    "display(outputlabel_1)\n",
    "\n",
    "inputbutton.on_click(handle_submit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
