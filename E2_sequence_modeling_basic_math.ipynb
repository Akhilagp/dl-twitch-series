{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Modeling: Train a model to do basic math\n",
    "\n",
    "@sunilmallya\n",
    "\n",
    "@jrhunt\n",
    "\n",
    "code will be available after session @ https://github.com/sunilmallya/dl-twitch-series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "We know how to generate basic math sequences, our input looks like this:\n",
    "\n",
    "5+4+9 = 18\n",
    "\n",
    "1+4+2 = 7 \n",
    "\n",
    "3 * 2 * 5 = 30\n",
    "\n",
    "9 * 2 * 2 = 36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNN)\n",
    "\n",
    "Certain problems require you to model information dependency. They can be external or could be based on previous context, which we'll explore in this notebook. Recurrent neural networks (RNN) are designed to address this issue. They are networks with loops in them, thus allowing information to persist, making them ideal to model sequences. Speech recognition, image captioning, video analysis, language modeling and many other use case can be addressed using RNN's. Check out Andrej Karapthy's [blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) for more details. Lets visualize a few examples of sequence in input or oputput: \n",
    "\n",
    "![rnn](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "<sub> img src: http://karpathy.github.io\n",
    "\n",
    "A RNN can be imagined as a repeated copy of the same network, each passing a message to the next step, in this case it may be to the same layer. But RNN's have problems with long range dependencies - interactions between sequences that are many steps apart, this is explained in detail in this [blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/). Also RNN's tend to be deep and hence more vulnerable to the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) \n",
    "\n",
    "To address these issues [Hochreiter & Schmidhuber (1997)](http://www.bioinf.jku.at/publications/older/2604.pdf) introduced LSTMs, which now are widely used to model the problems described above. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence modeling basics\n",
    "\n",
    "### What is sequence modeling?\n",
    "\n",
    "![](https://indico.io/blog/wp-content/uploads/2016/04/seq-nathan-fig3a.jpg)\n",
    "\n",
    "### Encoder - Decoder\n",
    "\n",
    "\n",
    "![](https://indico.io/blog/wp-content/uploads/2016/04/seq-nathan-figure3_b.jpg)\n",
    "\n",
    "Encoder transforms the input in to a hidden state. This can now be translated or converted in to any desirable form. The decoder tries to\n",
    "predict the next word in the outpute (decoder) sequence, given the current word in the decoder sequence and the context from the encoder sequence.\n",
    "\n",
    "### What problems can we solve with this\n",
    "\n",
    "- Language translation\n",
    "- image caption\n",
    "- sequence conversion\n",
    "\n",
    "\n",
    "<small>img src: https://indico.io/blog/sequence-modeling-neuralnets-part1/ </small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### why one hot ?\n",
    "\n",
    "In general most ML agorithms don't understand the label data directly. They like input/output variables to be numbers.\n",
    "\n",
    "For categorical data where there is no ordering, its not desirable to let the model assume any kind of ordering. One-hot encoding can help establish this and is a more desirable format for the neural network to work on.\n",
    "\n",
    "https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Its inspired by serveral blogs, but mainly from this paper - \"Sequence to Sequence Learning with Neural Networks\"\n",
    "\n",
    "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "\n",
    "import random\n",
    "random.seed(10)\n",
    "\n",
    "n_samples = 10000\n",
    "n_numbers = 3 # numbers to operate on\n",
    "largest = 10\n",
    "\n",
    "character_set = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', '*', ' ']\n",
    "\n",
    "input_sequence_length =  8 # (10 + 10 + 10)\n",
    "output_sequence_length = 4 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 8) (10000, 4)\n"
     ]
    }
   ],
   "source": [
    "def generate_data(n_samples):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    \n",
    "    char_to_int = dict((c,i)  for i,c in enumerate(character_set))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        lhs = [random.randint(1, largest) for _ in range(n_numbers)]\n",
    "        op = random.choice(['+', '*'])\n",
    "        if op == '+':\n",
    "            rhs = sum(lhs)\n",
    "        elif op == '*':\n",
    "            rhs = 1\n",
    "            for l in lhs:\n",
    "                rhs *= l\n",
    "        \n",
    "        lhs = [str(l) for l in lhs]\n",
    "        strng = op.join(lhs)\n",
    "        padded_strng = \"%*s\" % (input_sequence_length, strng)\n",
    "        enc_input = [char_to_int[ch] for ch in padded_strng]\n",
    "        \n",
    "        #RHS\n",
    "        padded_strng = \"%*s\" % (output_sequence_length, rhs)\n",
    "        enc_lbl = [char_to_int[ch] for ch in padded_strng]\n",
    "    \n",
    "        inputs.append(enc_input)\n",
    "        labels.append(enc_lbl)\n",
    "        \n",
    "    return np.array(inputs), np.array(labels)\n",
    "\n",
    "\n",
    "dataX, dataY = generate_data(n_samples)\n",
    "\n",
    "print dataX.shape, dataY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([DataDesc[data,(32, 8L),<type 'numpy.float32'>,NCHW]],\n",
       " [DataDesc[target,(32, 4L),<type 'numpy.float32'>,NCHW]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterators\n",
    "\n",
    "batch_size = 32\n",
    "data_dim = len(character_set)\n",
    "\n",
    "train_iter = mx.io.NDArrayIter(data=dataX, label=dataY,\n",
    "                               data_name='data', label_name='target',\n",
    "                               batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_iter.provide_data, train_iter.provide_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets build the model!\n",
    "\n",
    "data = mx.sym.var('data')\n",
    "target = mx.sym.var('target')\n",
    "\n",
    "# Encoder - Decoder \n",
    "\n",
    "lstm1 = mx.rnn.FusedRNNCell(num_hidden=32, prefix=\"lstm1_\", get_next_state=True)\n",
    "lstm2 = mx.rnn.FusedRNNCell(num_hidden=32, prefix=\"lstm2_\", get_next_state=False)\n",
    "\n",
    "# convert to one-hot encoding\n",
    "\n",
    "data_one_hot = mx.sym.one_hot(data, depth=len(character_set))\n",
    "data_one_hot = mx.sym.transpose(data_one_hot, axes=(1,0,2))\n",
    "\n",
    "# unroll the loop/lstm\n",
    "\n",
    "# Note that when unrolling, if 'merge_outputs' is set to True, the 'outputs' is merged into a single symbol\n",
    "# In the layout, 'N' represents batch size, 'T' represents sequence length, and 'C' represents the\n",
    "# number of dimensions in hidden states.\n",
    "\n",
    "l_out, encode_state = lstm1.unroll(length=input_sequence_length, inputs=data_one_hot, layout=\"TNC\")\n",
    "encode_state_h = mx.sym.broadcast_to(encode_state[0], shape=(output_sequence_length, 0, 0))\n",
    "\n",
    "# Decoder\n",
    "\n",
    "decode_out, l2 = lstm2.unroll(length=output_sequence_length, inputs=encode_state_h, layout=\"TNC\")\n",
    "decode_out = mx.sym.reshape(decode_out, shape=(-1,batch_size))\n",
    "\n",
    "out = mx.sym.FullyConnected(decode_out, num_hidden=data_dim)\n",
    "out = mx.sym.reshape(out, shape=(output_sequence_length, -1, data_dim))\n",
    "out = mx.sym.transpose(out, axes=(1,0,2))\n",
    "\n",
    "loss = mx.sym.mean(-mx.sym.pick(mx.sym.log_softmax(out), target, axis=-1))\n",
    "loss  = mx.sym.make_loss(loss)\n",
    "\n",
    "shape = {\"data\": (batch_size, dataX[0].shape[0])}\n",
    "#mx.viz.plot_network(out, shape=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#[\"cats\", \"dogs\"] ==> [0, 1] ==> [[1, 0], [0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Module\n",
    "\n",
    "\n",
    "net = mx.mod.Module(symbol=loss,\n",
    "                   data_names=['data'], label_names=['target'],\n",
    "                    context=mx.gpu(7)\n",
    "                   )\n",
    "\n",
    "net.bind(data_shapes=train_iter.provide_data, label_shapes=train_iter.provide_label)\n",
    "net.init_params(initializer=mx.init.Xavier())\n",
    "net.init_optimizer(optimizer='adam',\n",
    "                  optimizer_params={'learning_rate': 1E-3},\n",
    "                   kvstore=None\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.3908080\n",
      "1 0.9489369\n",
      "2 0.8515543\n",
      "3 0.7824094\n",
      "4 0.7301234\n",
      "5 0.6806088\n",
      "6 0.6317520\n",
      "7 0.5849520\n",
      "8 0.5348147\n",
      "9 0.4884985\n",
      "10 0.4454728\n",
      "11 0.4070407\n",
      "12 0.3729887\n",
      "13 0.3429023\n",
      "14 0.3168791\n",
      "15 0.2946095\n",
      "16 0.2736928\n",
      "17 0.2522101\n",
      "18 0.2382104\n",
      "19 0.2220520\n",
      "20 0.2054102\n",
      "21 0.1895113\n",
      "22 0.1765906\n",
      "23 0.1635755\n",
      "24 0.1484580\n",
      "25 0.1378150\n",
      "26 0.1310562\n",
      "27 0.1216568\n",
      "28 0.1063497\n",
      "29 0.0976198\n",
      "30 0.0942983\n",
      "31 0.0819174\n",
      "32 0.0779237\n",
      "33 0.0728757\n",
      "34 0.0661058\n",
      "35 0.0574462\n",
      "36 0.0595423\n",
      "37 0.0478818\n",
      "38 0.0763143\n",
      "39 0.0380665\n",
      "40 0.0342471\n",
      "41 0.0314132\n",
      "42 0.0370409\n",
      "43 0.0346601\n",
      "44 0.0237405\n",
      "45 0.0223978\n",
      "46 0.0493947\n",
      "47 0.0173508\n",
      "48 0.0156706\n",
      "49 0.0141608\n",
      "50 0.0129971\n",
      "51 0.0391612\n",
      "52 0.0143734\n",
      "53 0.0101149\n",
      "54 0.0091349\n",
      "55 0.0083722\n",
      "56 0.0077632\n",
      "57 0.0786065\n",
      "58 0.0402763\n",
      "59 0.0077026\n",
      "60 0.0066978\n",
      "61 0.0061074\n",
      "62 0.0056646\n",
      "63 0.0053165\n",
      "64 0.0050393\n",
      "65 0.0046588\n",
      "66 0.0044155\n",
      "67 0.0537669\n",
      "68 0.0054095\n",
      "69 0.0040832\n",
      "70 0.0037208\n",
      "71 0.0034542\n",
      "72 0.0032270\n",
      "73 0.0030249\n",
      "74 0.0028477\n",
      "75 0.0026960\n",
      "76 0.0025183\n",
      "77 0.0023169\n",
      "78 0.0021388\n",
      "79 0.0478033\n",
      "80 0.0035333\n",
      "81 0.0024058\n",
      "82 0.0021013\n",
      "83 0.0019403\n",
      "84 0.0018133\n",
      "85 0.0017037\n",
      "86 0.0016035\n",
      "87 0.0015086\n",
      "88 0.0014173\n",
      "89 0.0013297\n",
      "90 0.0012481\n",
      "91 0.0011718\n",
      "92 0.0011023\n",
      "93 0.0010249\n",
      "94 0.0698479\n",
      "95 0.0022211\n",
      "96 0.0013886\n",
      "97 0.0012301\n",
      "98 0.0011327\n",
      "99 0.0010600\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "total_batches = len(dataX) // batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    avg_loss =0\n",
    "    train_iter.reset()\n",
    "    \n",
    "    for i, data_batch in enumerate(train_iter):\n",
    "        net.forward_backward(data_batch=data_batch)\n",
    "        loss = net.get_outputs()[0].asscalar()\n",
    "        avg_loss += loss\n",
    "        net.update()\n",
    "    avg_loss /= total_batches\n",
    "\n",
    "    print epoch, \"%.7f\" % avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expression predicted actual\n",
      "   4+3+5   12        12\n",
      "   5*2*8   80        80\n",
      "   9*7*1   63        63\n",
      "   7+5+9   21        21\n",
      "   5+6+9   20        20\n",
      "   6*7*9  378       378\n",
      "   6+2+8   16        16\n",
      "   1*9*8   72        72\n",
      "   4+9+4   17        17\n",
      "   8+8+3   19        19\n",
      "   2*3*7   42        42\n",
      "   1+9+5   15        15\n",
      "   2+4+1    7         7\n",
      "   2*3*4   24        24\n",
      "   3+5+8   16        16\n",
      "   1*8*3   24        24\n",
      "   6*8*6  288       288\n",
      " 10+10+5   25        25\n",
      "  10+1+7   18        18\n",
      "  8+4+10   22        22\n",
      "   7+4+4   15        15\n",
      "   6*3*5   90        90\n",
      " 10*10*7  700       700\n",
      "  10+8+8   26        26\n",
      "   7*8*7  392       392\n",
      "   1*3*8   24        24\n",
      "   2*5*9   90        90\n",
      "  4*10*1   40        40\n",
      "   9*4*8  288       288\n",
      "   6+7+2   15        15\n",
      "  8*10*1   80        80\n",
      "   9+5+8   22        22\n",
      "   5+4+3   12        12\n",
      "   7*9*9  567       567\n",
      "  8*10*7  560       560\n",
      "   7+3+1   11        11\n",
      "   9*9*5  405       405\n",
      "   4*2*7   56        56\n",
      "   6+1+2    9         9\n",
      "  1+10+6   17        17\n",
      "   8*8*9  576       576\n",
      "   4+5+6   15        15\n",
      "   3*8*9  216       216\n",
      "   1*1*5    5         5\n",
      "   3*3*2   18        18\n",
      "  10*6*6  360       360\n",
      "  5*10*3  150       150\n",
      "  8+10+9   27        27\n",
      "   1*9*8   72        72\n",
      "   2*6*7   84        84\n",
      "   7+6+7   20        20\n",
      "   2*3*9   54        54\n",
      "  10*3*4  120       120\n",
      "   6*6*7  252       252\n",
      "   2+2+4    8         8\n",
      "  1*2*10   20        20\n",
      "   1+4+5   10        10\n",
      "   6+6+1   13        13\n",
      "  7+8+10   25        25\n",
      "   3*8*6  144       144\n",
      "   2*7*4   56        56\n",
      "  10+3+5   18        18\n",
      "   9+3+1   13        13\n",
      "   2*4*6   48        48\n",
      "   3+7+3   13        13\n",
      "   2*7*5   70        70\n",
      "   4*8*7  224       224\n",
      "   9+7+4   20        20\n",
      "   3+7+8   18        18\n",
      "   7*8*4  224       224\n",
      "   6*8*9  432       432\n",
      "   8*5*9  360       360\n",
      "   4+5+1   10        10\n",
      "   4+3+5   12        12\n",
      "   5*3*2   30        30\n",
      "  6*10*3  180       180\n",
      "  7+10+3   20        20\n",
      "   7*2*9  126       126\n",
      "   4+6+3   13        13\n",
      "  8*10*7  560       560\n",
      "   2*9*1   18        18\n",
      "   3+8+7   18        18\n",
      "   6+8+2   16        16\n",
      "   8*5*9  360       360\n",
      "   3+7+9   19        19\n",
      "   7*3*4   84        84\n",
      "   9*8*9  648       648\n",
      "  10+7+3   20        20\n",
      "   8+9+2   19        19\n",
      "   6*8*9  432       432\n",
      "   4+8+5   17        17\n",
      "   5*7*2   70        70\n",
      "   8+4+2   14        14\n",
      "   1*3*7   21        21\n",
      "   3*9*9  243       243\n",
      "   2*8*2   32        32\n",
      "   9*3*6  162       162\n",
      "   3*4*5   60        60\n",
      "   1*7*1    7         7\n",
      "   1+1+1    3         3\n",
      "100 1.0\n"
     ]
    }
   ],
   "source": [
    "# test module\n",
    "test_net = mx.mod.Module(symbol=out,\n",
    "                         data_names=['data'],\n",
    "                         label_names=None,\n",
    "                         context=mx.gpu(7)) # FusedRNNCell works only with GPU\n",
    "\n",
    "# data descriptor\n",
    "data_desc = train_iter.provide_data[0]\n",
    "\n",
    "# set shared_module = model used for training so as to share same parameters and memory\n",
    "test_net.bind(data_shapes=[data_desc],\n",
    "              label_shapes=None,\n",
    "              for_training=False,\n",
    "              grad_req='null',\n",
    "              shared_module=net)\n",
    "\n",
    "n_test = 100\n",
    "testX, testY = generate_data(n_test)\n",
    "\n",
    "testX = np.array(testX, dtype=np.int)\n",
    "\n",
    "test_net.reshape(data_shapes=[mx.io.DataDesc('data', (1, input_sequence_length))])\n",
    "predictions = test_net.predict(mx.io.NDArrayIter(testX, batch_size=1)).asnumpy()\n",
    "\n",
    "print \"expression\", \"predicted\", \"actual\"\n",
    "\n",
    "correct = 0\n",
    "for i, prediction in enumerate(predictions):\n",
    "    x_str = [character_set[j] for j in testX[i]]\n",
    "    index = np.argmax(prediction, axis=1)\n",
    "    result = [character_set[j] for j in index]\n",
    "    label = [character_set[j] for j in testY[i]]\n",
    "    #print result, label\n",
    "    if result == label:\n",
    "        correct +=1\n",
    "    print \"\".join(x_str), \"\".join(result), \"    \", \"\".join(label)\n",
    "    \n",
    "print correct, correct/(n_test*1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
